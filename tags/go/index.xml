<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>go on monique.dev</title><link>https://monique.dev/tags/go/</link><description>Recent content in go on monique.dev</description><generator>Hugo -- gohugo.io</generator><language>pt-br</language><lastBuildDate>Fri, 08 Jan 2021 12:01:00 -0300</lastBuildDate><atom:link href="https://monique.dev/tags/go/index.xml" rel="self" type="application/rss+xml"/><item><title>Memory efficient parsing in GO</title><link>https://monique.dev/posts/go-parsing/</link><pubDate>Fri, 08 Jan 2021 12:01:00 -0300</pubDate><guid>https://monique.dev/posts/go-parsing/</guid><description>Instead of greedily reading the whole stream in memory and passing it to json.Unmarshal:
json.Unmarshal(bigBlobOfData, &amp;amp;myStruct) We can delegate the stream reading to our dear masters of golang:
json.NewDecoder(myIOReader).Decode(&amp;amp;myStruct) And read multi giga(tera?)bytes files with no sweat. Also works with xml.NewDecoder.</description></item></channel></rss>